{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alpine-throat",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "> The **sklearn.preprocessing** package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-playback",
   "metadata": {},
   "source": [
    "### 1. Standardization\n",
    "*  Standardization, or mean removal and variance scaling.\n",
    "* Gaussian with zero mean and unit variance.\n",
    "\n",
    "### `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "civil-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valid-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15, -15,  17,   8],\n",
       "       [ 22,  33,  76,  99],\n",
       "       [ 45,  76, -98,  66],\n",
       "       [ 87,  45,   3,   5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([\n",
    "    [15, -15, 17, 8],\n",
    "    [22, 33, 76, 99],\n",
    "    [45, 76, -98, 66],\n",
    "    [87, 45, 3, 5]\n",
    "])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caroline-leader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "successful-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96908991, -1.52005862,  0.27953524, -0.91789414],\n",
       "       [-0.72014938, -0.0534694 ,  1.22196833,  1.37055426],\n",
       "       [ 0.09779806,  1.26035011, -1.55741062,  0.54067737],\n",
       "       [ 1.59144122,  0.31317791,  0.05590705, -0.99333749]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civilian-platinum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.25, 34.75, -0.5 , 44.5 ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liquid-instrument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.11916606, 32.7290009 , 62.60391362, 39.7649343 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-pride",
   "metadata": {},
   "source": [
    "> This class implements the Transformer API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a **Pipeline**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-beatles",
   "metadata": {},
   "source": [
    "### `MinMaxScaler` and `MaxAbsScaler`\n",
    "* he motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gothic-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controversial-colors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.66091954, 0.03191489],\n",
       "       [0.09722222, 0.52747253, 1.        , 1.        ],\n",
       "       [0.41666667, 1.        , 0.        , 0.64893617],\n",
       "       [1.        , 0.65934066, 0.58045977, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = MinMaxScaler()\n",
    "scale.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-burst",
   "metadata": {},
   "source": [
    "> **Training range lies between `0 and 1`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appointed-utilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17241379, -0.19736842,  0.17346939,  0.08080808],\n",
       "       [ 0.25287356,  0.43421053,  0.7755102 ,  1.        ],\n",
       "       [ 0.51724138,  1.        , -1.        ,  0.66666667],\n",
       "       [ 1.        ,  0.59210526,  0.03061224,  0.05050505]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = MaxAbsScaler()\n",
    "scale.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-proceeding",
   "metadata": {},
   "source": [
    "> **Training range lies between `-1 and 1`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-juice",
   "metadata": {},
   "source": [
    "### Scaling sparse data\n",
    "Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales.\n",
    "\n",
    "**``MaxAbsScaler``** was specifically designed for scaling ``sparse data`` (**Typically, sparse data means that there are many gaps present in the data being recorded.**), and is the recommended way to go about this. However, **``StandardScaler``** can accept ``scipy.sparse`` matrices as input, as long as ``with_mean=False`` is explicitly passed to the constructor. Otherwise a ``ValueError`` will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
